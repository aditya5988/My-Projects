{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b1e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e265e98",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb5fbd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 stop words :  ['can', 'an', 'herself', 'put', 'nowhere', '‘d', 'mine', 'often', 'hereafter', 'these', 'otherwise', 'within', 'without', 'could', 'nine']\n",
      "\n",
      "Total no of stop words in spacy :  326\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords=spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "print('First 15 stop words : ' ,list(spacy_stopwords)[:15])\n",
    "\n",
    "print('\\nTotal no of stop words in spacy : ',len(spacy_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7fb1c0",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0cbcbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cries -> cri\n",
      "this -> this\n",
      "lied -> lie\n",
      "computing -> comput\n",
      "organizing -> organ\n",
      "matches -> match\n"
     ]
    }
   ],
   "source": [
    "tokens=['cries','this','lied','computing','organizing','matches']\n",
    "\n",
    "# Doing Stemming\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer=SnowballStemmer(language='english')\n",
    "for token in tokens:\n",
    "        print(token + ' -> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd9f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cries  ->  cry\n",
      "this  ->  this\n",
      "lied  ->  lie\n",
      "computing  ->  computing\n",
      "organizing  ->  organizing\n",
      "matches  ->  match\n"
     ]
    }
   ],
   "source": [
    "# Doing Lemmatization\n",
    "\n",
    "Token='cries this lied computing organizing matches'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(Token)\n",
    "for word in doc:\n",
    "    print(word.text, ' -> ',word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3102cbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stemming :  cries -> cri\n",
      "After Stemming :  this -> this\n",
      "After Stemming :  lied -> lie\n",
      "After Stemming :  computing -> comput\n",
      "After Stemming :  organizing -> organ\n",
      "After Stemming :  matches -> match\n",
      "\n",
      "After Lemmatization :  cries -> cry\n",
      "After Lemmatization :  this -> this\n",
      "After Lemmatization :  lied -> lie\n",
      "After Lemmatization :  computing -> computing\n",
      "After Lemmatization :  organizing -> organizing\n",
      "After Lemmatization :  matches -> match\n"
     ]
    }
   ],
   "source": [
    "# Comparing results of Lemmatization and Stemming\n",
    "\n",
    "for token in tokens:\n",
    "    print('After Stemming : ',token + ' -> ' + stemmer.stem(token))\n",
    "\n",
    "print()\n",
    "\n",
    "for word in doc:\n",
    "    print('After Lemmatization : ',word.text + ' -> ' + word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e11f1",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dbda969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  a)\n",
    "\n",
    "file=open('scifiscripts_intro.txt').read()\n",
    "# file\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "ax=[]\n",
    "doc=nlp(str(file))\n",
    "for word in doc:\n",
    "    if word.is_stop==False:\n",
    "        ax.append(word)\n",
    "print('After removing the stop words :\\n ',ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea0dd6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after removal of stop word :  [smart, intelligent, mind]\n",
      "Output after removal of stop word :  [comes, late, class]\n",
      "Output after removal of stop word :  [Tommorrow, big, day, students]\n",
      "Output after removal of stop word :  [prefer, reading, books, free, time]\n",
      "Output after removal of stop word :  [World, tallest, building, burj, khalifa, dubai]\n"
     ]
    }
   ],
   "source": [
    "#  b)\n",
    "\n",
    "a='He is very smart and intelligent by mind'\n",
    "b='He always comes late in class'\n",
    "c='Tommorrow is big day for the students'\n",
    "d='I always prefer reading books in free time'\n",
    "e='World tallest building is burj khalifa is in dubai'\n",
    "\n",
    "s1=nlp(a)\n",
    "res1=[]\n",
    "for a1 in s1:\n",
    "    if a1.is_stop==False:\n",
    "        res1.append(a1)\n",
    "print('Output after removal of stop word : ',res1)\n",
    "\n",
    "s2=nlp(b)\n",
    "res2=[]\n",
    "for b1 in s2:\n",
    "    if b1.is_stop==False:\n",
    "        res2.append(b1)\n",
    "print('Output after removal of stop word : ',res2)\n",
    "\n",
    "s3=nlp(c)\n",
    "res3=[]\n",
    "for c1 in s3:\n",
    "    if c1.is_stop==False:\n",
    "        res3.append(c1)\n",
    "print('Output after removal of stop word : ',res3)\n",
    "\n",
    "s4=nlp(d)\n",
    "res4=[]\n",
    "for d1 in s4:\n",
    "    if d1.is_stop==False:\n",
    "        res4.append(d1)\n",
    "print('Output after removal of stop word : ',res4)\n",
    "    \n",
    "\n",
    "s5=nlp(e)\n",
    "res5=[]\n",
    "for e1 in s5:\n",
    "    if e1.is_stop==False:\n",
    "        res5.append(e1)\n",
    "print('Output after removal of stop word : ',res5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a135a7",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f38bcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bryan \t\t PROPN \t\t NNP\n",
      "visited \t\t VERB \t\t VBD\n",
      "his \t\t PRON \t\t PRP$\n",
      "friend \t\t NOUN \t\t NN\n",
      "for \t\t ADP \t\t IN\n",
      "a \t\t DET \t\t DT\n",
      "while \t\t NOUN \t\t NN\n",
      "and \t\t CCONJ \t\t CC\n",
      "then \t\t ADV \t\t RB\n",
      "went \t\t VERB \t\t VBD\n",
      "home \t\t ADV \t\t RB\n",
      "at \t\t ADP \t\t IN\n",
      "10 \t\t NUM \t\t CD\n",
      "pm \t\t NOUN \t\t NN\n"
     ]
    }
   ],
   "source": [
    "text='Bryan visited his friend for a while and then went home at 10pm'\n",
    "\n",
    "doc=nlp(text)\n",
    "for word in doc:\n",
    "    print(word.text,'\\t\\t',word.pos_,'\\t\\t',word.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b11b24",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df08c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun are :  [PADUA, HIGH, SCHOOL, DAY, Revision, November]\n",
      "Number are :  [12, 1997]\n"
     ]
    }
   ],
   "source": [
    "file=open('Random.txt').read()\n",
    "# file\n",
    "\n",
    "doc=nlp(file)\n",
    "noun=[]\n",
    "num=[]\n",
    "for word in doc:\n",
    "    if word.pos_ == 'PROPN':\n",
    "        noun.append(word)\n",
    "    elif word.pos_ == 'NUM':\n",
    "        num.append(word)\n",
    "        \n",
    "print('Noun are : ',noun)\n",
    "print('Number are : ',num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc42200",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68121a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding stop words : \n",
      "\n",
      "{'can', 'an', 'herself', 'put', 'nowhere', '‘d', 'mine', 'often', 'hereafter', 'these', 'otherwise', 'within', 'without', 'could', 'bye', 'nine', 'perhaps', 'among', 'such', 'together', 'one', 'though', 'wherein', 'however', 'that', 'other', 'further', 'thus', 'seemed', 'about', 'serious', 'except', 'by', 'whatever', 'still', 'upon', 'her', 'latterly', 'top', 'nevertheless', 'name', 'after', 'his', 'around', 'sometime', 'who', 'up', 'whereafter', 'across', 'himself', 'under', \"n't\", '‘s', 'through', 'to', 'empty', 'former', 'their', 'hence', 'below', 'twelve', 'above', 'ever', 'because', 'say', 'whether', 'very', 'never', 'there', 'ourselves', 'give', 'at', 'somehow', 'our', 'while', 'much', 'hereby', 'take', 'whereas', '‘ve', 'hundred', 'least', 'seems', 'foolish', 'almost', 'being', \"'re\", 'toward', '’s', 'your', 'side', 'therein', 'too', 'ten', 'may', 'just', 'yours', 'itself', 'cannot', 'per', 'everyone', 'yourselves', 'i', 'yet', 'becoming', 'whither', 'anyhow', 'does', 'them', 'wish', \"'s\", 'n‘t', 'seeming', 'nobody', 'into', 'in', 'should', 'she', '’m', 'alone', 'on', 'down', 'as', 'enough', 'whence', 'therefore', 'three', 'either', 'which', 'not', '’d', 'against', 'yourself', 'please', 'do', 'were', 'neither', 'amount', 'become', 'no', 'others', 'many', 'more', 'five', 'whole', 'why', 'thru', 'same', 'anywhere', 'between', 'full', 'sopreety', 'nor', 'unless', 'but', 'eight', 'throughout', 'moreover', 'noone', 'be', \"'ll\", 'had', 'would', 'few', 'whom', 'will', 'move', 'since', 'twenty', 'my', 'we', 'herein', 'is', 'each', 're', 'beside', 'where', 'themselves', 'wherever', 'mostly', 'another', 'ca', 'than', 'latter', 'although', 'are', 'only', 'amongst', 'anything', 'whenever', 'all', 'two', 'due', 'make', 'first', 'else', 'have', 'front', 'made', 'been', 'four', 'whoever', 'every', 'indeed', 'well', 'did', 'via', 'he', 'hers', 'how', 'then', \"'m\", 'him', 'whereupon', 'already', 'regarding', \"'d\", 'always', 'thence', 'here', 'get', 'seem', 'ours', 'for', 'somewhere', 'along', 'see', 'using', 'any', 'once', 'both', 'rather', 'it', 'last', 'forty', 'none', 'really', 'of', 'anyway', 'various', 'doing', 'from', 'several', '‘ll', 'used', 'also', '’re', 'if', 'eleven', '’ve', 'meanwhile', 'has', 'fifty', 'someone', 'some', 'call', '‘m', 'so', 'thereby', 'when', 'thereafter', 'was', 'hereupon', 'towards', 'even', 'now', 'formerly', 'everything', 'me', 'quite', 'during', 'or', 'you', 'fifteen', 'before', '’ll', 'the', 'third', 'with', 'those', 'elsewhere', 'part', 'becomes', 'something', '‘re', 'bottom', 'less', 'show', 'beyond', 'myself', 'became', 'and', 'out', 'own', 'everywhere', 'off', 'thereupon', 'onto', 'us', 'besides', 'most', 'whose', 'back', 'sometimes', 'again', 'they', 'its', 'keep', 'might', 'done', 'behind', 'beforehand', 'over', 'whereby', 'namely', 'n’t', 'go', 'until', 'nothing', 'am', \"'ve\", 'sixty', 'next', 'must', 'this', 'a', 'what', 'anyone', 'misabuse', 'six', 'afterwards'}\n"
     ]
    }
   ],
   "source": [
    "# adding 5 stop words\n",
    "\n",
    "nlp.Defaults.stop_words |= {'bye','wish','sopreety','foolish','misabuse'}\n",
    "print('After adding stop words : \\n')\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb654357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing stop words :\n",
      "\n",
      "{'can', 'an', 'herself', 'put', 'nowhere', '‘d', 'mine', 'often', 'hereafter', 'these', 'otherwise', 'within', 'without', 'could', 'bye', 'nine', 'perhaps', 'among', 'such', 'together', 'one', 'though', 'wherein', 'however', 'that', 'other', 'further', 'thus', 'seemed', 'about', 'serious', 'except', 'by', 'whatever', 'still', 'upon', 'her', 'latterly', 'top', 'nevertheless', 'name', 'after', 'his', 'around', 'sometime', 'who', 'up', 'whereafter', 'across', 'himself', 'under', \"n't\", '‘s', 'through', 'to', 'empty', 'former', 'their', 'hence', 'below', 'twelve', 'above', 'ever', 'because', 'say', 'whether', 'very', 'there', 'ourselves', 'give', 'at', 'somehow', 'our', 'while', 'much', 'hereby', 'take', 'whereas', '‘ve', 'hundred', 'least', 'seems', 'foolish', 'almost', 'being', \"'re\", 'toward', '’s', 'your', 'side', 'therein', 'too', 'ten', 'may', 'just', 'yours', 'itself', 'cannot', 'per', 'everyone', 'yourselves', 'i', 'yet', 'becoming', 'whither', 'anyhow', 'does', 'them', 'wish', \"'s\", 'n‘t', 'seeming', 'nobody', 'into', 'in', 'should', 'she', '’m', 'alone', 'on', 'down', 'as', 'enough', 'whence', 'therefore', 'three', 'either', 'which', 'not', '’d', 'against', 'yourself', 'please', 'do', 'were', 'neither', 'amount', 'become', 'no', 'others', 'many', 'more', 'five', 'whole', 'why', 'thru', 'same', 'anywhere', 'full', 'sopreety', 'nor', 'unless', 'but', 'eight', 'throughout', 'moreover', 'noone', 'be', \"'ll\", 'had', 'would', 'few', 'whom', 'will', 'move', 'since', 'twenty', 'my', 'we', 'herein', 'is', 'each', 're', 'beside', 'where', 'themselves', 'wherever', 'mostly', 'another', 'ca', 'than', 'latter', 'although', 'are', 'only', 'amongst', 'anything', 'whenever', 'all', 'two', 'due', 'make', 'first', 'else', 'have', 'front', 'made', 'been', 'four', 'whoever', 'every', 'indeed', 'well', 'did', 'via', 'he', 'hers', 'how', 'then', \"'m\", 'him', 'whereupon', 'already', 'regarding', \"'d\", 'thence', 'here', 'get', 'seem', 'ours', 'for', 'somewhere', 'along', 'see', 'using', 'any', 'once', 'both', 'rather', 'it', 'last', 'forty', 'none', 'really', 'of', 'anyway', 'various', 'doing', 'from', 'several', '‘ll', 'used', 'also', '’re', 'if', 'eleven', '’ve', 'meanwhile', 'has', 'fifty', 'someone', 'some', 'call', '‘m', 'so', 'thereby', 'when', 'thereafter', 'was', 'hereupon', 'towards', 'even', 'now', 'formerly', 'everything', 'me', 'quite', 'during', 'or', 'you', 'fifteen', 'before', '’ll', 'the', 'third', 'with', 'those', 'elsewhere', 'part', 'something', '‘re', 'bottom', 'less', 'show', 'beyond', 'myself', 'became', 'and', 'out', 'own', 'everywhere', 'off', 'thereupon', 'onto', 'us', 'besides', 'most', 'whose', 'back', 'sometimes', 'again', 'they', 'its', 'keep', 'might', 'done', 'behind', 'beforehand', 'over', 'whereby', 'namely', 'n’t', 'go', 'until', 'nothing', 'am', \"'ve\", 'sixty', 'next', 'must', 'this', 'a', 'what', 'anyone', 'misabuse', 'six', 'afterwards'}\n"
     ]
    }
   ],
   "source": [
    "# removing stop words\n",
    "\n",
    "nlp.Defaults.stop_words -= {'always','never','between','becomes'}\n",
    "print('After removing stop words :\\n')\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723505f",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2470c348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PADUA HIGH SCHOOL - DAY\\nRevision November 12, 1997\\nI hope dinner's ready because I only have ten minutes before Mrs. Johnson squirts out a screamer.\\nHe grabs the mail and rifles through it, as he bends down to kiss Sharon on the cheek.\\nMICHAEL- C'mon. I'm supposed to give you the tour. They head out of the office\\nMICHAEL (continuing)- So -- which Dakota you from?\\n          \\n                                 \\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=open('Raw_data_for_analysis.txt').read()\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5112a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PADUA', 'HIGH', 'SCHOOL', '-', 'DAY', '\\n', 'Revision', 'November', '12', ',', '1997', '\\n', 'I', 'hope', 'dinner', \"'s\", 'ready', 'because', 'I', 'only', 'have', 'ten', 'minutes', 'before', 'Mrs.', 'Johnson', 'squirts', 'out', 'a', 'screamer', '.', '\\n', 'He', 'grabs', 'the', 'mail', 'and', 'rifles', 'through', 'it', ',', 'as', 'he', 'bends', 'down', 'to', 'kiss', 'Sharon', 'on', 'the', 'cheek', '.', '\\n', 'MICHAEL-', \"C'm\", 'on', '.', 'I', \"'m\", 'supposed', 'to', 'give', 'you', 'the', 'tour', '.', 'They', 'head', 'out', 'of', 'the', 'office', '\\n', 'MICHAEL', '(', 'continuing)-', 'So', '--', 'which', 'Dakota', 'you', 'from', '?', '\\n          \\n                                 \\n']\n"
     ]
    }
   ],
   "source": [
    "# Applying Tokenization\n",
    "\n",
    "token=[]\n",
    "doc=nlp(str(file))\n",
    "for toke in doc:\n",
    "    token.append(toke.text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68104a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[, ', PADUA, ', ,, ', HIGH, ', ,, ', SCHOOL, ', ,, ', -, ', ,, ', DAY, ', ,, ', \\n, ', ,, ', Revision, ', ,, ', November, ', ,, ', 12, ', ,, ', ,, ', ,, ', 1997, ', ,, ', \\n, ', ,, ', ', ,, ', hope, ', ,, ', dinner, ', ,, \", \", ,, ', ready, ', ,, ', ', ,, ', ', ,, ', ', ,, ', ', ,, ', ', ,, ', minutes, ', ,, ', ', ,, ', Mrs., ', ,, ', Johnson, ', ,, ', squirts, ', ,, ', ', ,, ', ', ,, ', screamer, ', ,, ', ., ', ,, ', \\n, ', ,, ', ', ,, ', grabs, ', ,, ', ', ,, ', mail, ', ,, ', ', ,, ', rifles, ', ,, ', ', ,, ', ', ,, ', ,, ', ,, ', ', ,, ', ', ,, ', bends, ', ,, ', ', ,, ', ', ,, ', kiss, ', ,, ', Sharon, ', ,, ', ', ,, ', ', ,, ', cheek, ', ,, ', ., ', ,, ', \\n, ', ,, ', MICHAEL-, ', ,, \", C'm, \", ,, ', ', ,, ', ., ', ,, ', ', ,, \", ', m, \", ,, ', supposed, ', ,, ', ', ,, ', ', ,, ', ', ,, ', ', ,, ', tour, ', ,, ', ., ', ,, ', ', ,, ', head, ', ,, ', ', ,, ', ', ,, ', ', ,, ', office, ', ,, ', \\n, ', ,, ', MICHAEL, ', ,, ', (, ', ,, ', continuing)-, ', ,, ', ', ,, ', --, ', ,, ', ', ,, ', Dakota, ', ,, ', ', ,, ', ', ,, ', ?, ', ,, ', \\n,          , \\n,                                 , \\n, ', ]]\n"
     ]
    }
   ],
   "source": [
    "# Removing stop word from input\n",
    "\n",
    "# spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS  \n",
    "filter_doc=[]\n",
    "doc=nlp(str(token))\n",
    "for word in doc:\n",
    "    if word.is_stop==False:\n",
    "        filter_doc.append(word)\n",
    "print(filter_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9070ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  ->  [\n",
      "[  ->  [\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "PADUA  ->  padua\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "HIGH  ->  high\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "SCHOOL  ->  SCHOOL\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "-  ->  -\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "DAY  ->  DAY\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "\\n  ->  \\n\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "Revision  ->  Revision\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "November  ->  November\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "12  ->  12\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "1997  ->  1997\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "\\n  ->  \\n\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "hope  ->  hope\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "dinner  ->  dinner\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "\"  ->  \"\n",
      ",  ->  ,\n",
      "\"  ->  \"\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "ready  ->  ready\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "minutes  ->  minute\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "Mrs.  ->  Mrs.\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "Johnson  ->  Johnson\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "squirts  ->  squirt\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "screamer  ->  screamer\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ".  ->  .\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "\\n  ->  \\n\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "grabs  ->  grab\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "mail  ->  mail\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "rifles  ->  rifle\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "bends  ->  bend\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "kiss  ->  kiss\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "Sharon  ->  Sharon\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "cheek  ->  cheek\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ".  ->  .\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "\\n  ->  \\n\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "MICHAEL-  ->  MICHAEL-\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "\"  ->  \"\n",
      ",  ->  ,\n",
      "C'm  ->  c'm\n",
      ",  ->  ,\n",
      "\"  ->  \"\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ".  ->  .\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "\"  ->  \"\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "m  ->  m\n",
      ",  ->  ,\n",
      "\"  ->  \"\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "supposed  ->  suppose\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "tour  ->  tour\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ".  ->  .\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "head  ->  head\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "office  ->  office\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "\\n  ->  \\n\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "MICHAEL  ->  MICHAEL\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "(  ->  (\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "continuing)-  ->  continuing)-\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "--  ->  --\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "Dakota  ->  Dakota\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "?  ->  ?\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "\\n  ->  \\n\n",
      ",  ->  ,\n",
      "           ->           \n",
      ",  ->  ,\n",
      "\\n  ->  \\n\n",
      ",  ->  ,\n",
      "                                  ->                                  \n",
      ",  ->  ,\n",
      "\\n  ->  \\n\n",
      ",  ->  ,\n",
      "'  ->  '\n",
      ",  ->  ,\n",
      "]  ->  ]\n",
      "]  ->  ]\n"
     ]
    }
   ],
   "source": [
    "# Apply lemmatization\n",
    "\n",
    "doc1=nlp(str(filter_doc))\n",
    "for word in doc1:\n",
    "        print(word.text ,' -> ',word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28c2cc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA \t\t PROPN \t\t NNP\n",
      "HIGH \t\t PROPN \t\t NNP\n",
      "SCHOOL \t\t PROPN \t\t NNP\n",
      "- \t\t PUNCT \t\t HYPH\n",
      "DAY \t\t PROPN \t\t NNP\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "Revision \t\t PROPN \t\t NNP\n",
      "November \t\t PROPN \t\t NNP\n",
      "12 \t\t NUM \t\t CD\n",
      ", \t\t PUNCT \t\t ,\n",
      "1997 \t\t NUM \t\t CD\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "I \t\t PRON \t\t PRP\n",
      "hope \t\t VERB \t\t VBP\n",
      "dinner \t\t NOUN \t\t NN\n",
      "'s \t\t PART \t\t POS\n",
      "ready \t\t ADJ \t\t JJ\n",
      "because \t\t SCONJ \t\t IN\n",
      "I \t\t PRON \t\t PRP\n",
      "only \t\t ADV \t\t RB\n",
      "have \t\t VERB \t\t VBP\n",
      "ten \t\t NUM \t\t CD\n",
      "minutes \t\t NOUN \t\t NNS\n",
      "before \t\t SCONJ \t\t IN\n",
      "Mrs. \t\t PROPN \t\t NNP\n",
      "Johnson \t\t PROPN \t\t NNP\n",
      "squirts \t\t VERB \t\t VBZ\n",
      "out \t\t ADP \t\t RP\n",
      "a \t\t DET \t\t DT\n",
      "screamer \t\t NOUN \t\t NN\n",
      ". \t\t PUNCT \t\t .\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "He \t\t PRON \t\t PRP\n",
      "grabs \t\t VERB \t\t VBZ\n",
      "the \t\t DET \t\t DT\n",
      "mail \t\t NOUN \t\t NN\n",
      "and \t\t CCONJ \t\t CC\n",
      "rifles \t\t NOUN \t\t NNS\n",
      "through \t\t ADP \t\t IN\n",
      "it \t\t PRON \t\t PRP\n",
      ", \t\t PUNCT \t\t ,\n",
      "as \t\t SCONJ \t\t IN\n",
      "he \t\t PRON \t\t PRP\n",
      "bends \t\t VERB \t\t VBZ\n",
      "down \t\t ADP \t\t RP\n",
      "to \t\t PART \t\t TO\n",
      "kiss \t\t VERB \t\t VB\n",
      "Sharon \t\t PROPN \t\t NNP\n",
      "on \t\t ADP \t\t IN\n",
      "the \t\t DET \t\t DT\n",
      "cheek \t\t NOUN \t\t NN\n",
      ". \t\t PUNCT \t\t .\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "MICHAEL- \t\t PROPN \t\t NNP\n",
      "C'm \t\t VERB \t\t VBZ\n",
      "on \t\t ADP \t\t RP\n",
      ". \t\t PUNCT \t\t .\n",
      "I \t\t PRON \t\t PRP\n",
      "'m \t\t AUX \t\t VBP\n",
      "supposed \t\t VERB \t\t VBN\n",
      "to \t\t PART \t\t TO\n",
      "give \t\t VERB \t\t VB\n",
      "you \t\t PRON \t\t PRP\n",
      "the \t\t DET \t\t DT\n",
      "tour \t\t NOUN \t\t NN\n",
      ". \t\t PUNCT \t\t .\n",
      "They \t\t PRON \t\t PRP\n",
      "head \t\t VERB \t\t VBP\n",
      "out \t\t ADP \t\t IN\n",
      "of \t\t ADP \t\t IN\n",
      "the \t\t DET \t\t DT\n",
      "office \t\t NOUN \t\t NN\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "MICHAEL \t\t PROPN \t\t NNP\n",
      "( \t\t PUNCT \t\t -LRB-\n",
      "continuing)- \t\t NOUN \t\t NNS\n",
      "So \t\t ADV \t\t RB\n",
      "-- \t\t PUNCT \t\t :\n",
      "which \t\t PRON \t\t WDT\n",
      "Dakota \t\t PROPN \t\t NNP\n",
      "you \t\t PRON \t\t PRP\n",
      "from \t\t ADP \t\t IN\n",
      "? \t\t PUNCT \t\t .\n",
      "\n",
      "          \n",
      "                                 \n",
      " \t\t SPACE \t\t _SP\n"
     ]
    }
   ],
   "source": [
    "# Applying pos tagging\n",
    "\n",
    "doc2=nlp(str(file))\n",
    "for word in doc2:\n",
    "    print(word.text,'\\t\\t',word.pos_,'\\t\\t',word.tag_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
